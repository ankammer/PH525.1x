Questions Week 3 Inference III


Using Monte Carlo to examine the sample variance

In the video, Rafa showed how to examine the null distribution of the t-

statistic using Monte Carlo simulations: by randomly drawing 2 sets of n samples 

from the non-smokers group and then calculating the t-statistic. We could then 

see that this distribution was approximately normal when n was large, but not 

when n was small (n=3).

In this assessment, we will use the same technique to examine the sample 

variance. A quick reminder: on the sample variance, if we have a sample 'x' with 

length 'n', the sample variance is defined:

1/(n-1) * sum( ( x - mean(x) )^2 )

In R, we can simply write

var(x)

The sample variance will not typically be equal to the population (in other 

words, it will have some error), and the sample variance depends on the sample, 

which is random, so the sample variance is a random variable. Here we will 

examine how the sample variance is likely to be closer to the true variance as 

the size of the sample increases. Let's start by loading the "babies" data that 

was used in the previous video. The data can be downloaded from here.

babies = read.table("babies.txt", header=TRUE)

The population of nonsmoker baby weights is:

bwt.nonsmoke = babies$bwt[babies$smoke==0]

And the population variance is 302.7144:

pop.var = var(bwt.nonsmoke)

Question 1.1 

Use replicate() to perform Monte Carlo simulations. So the following line of 

example code will repeat the R expression, "xyz" 1000 times (you need to fill in 

your R code in place of "xyz"):

vars = replicate(1000, xyz)

Using Monte Carlo simulation, take 1000 samples of size 10 from bwt.nonsmoke and 

calculate the variance. Look at a histogram of vars, and add the population 

variance as a vertical line.

How often (what proportion of simulations) is the sample variance greater than 

1.5 times the population variance? Use 1000 simulations. Your answer will 

change, but we have used a range of permissible answers. For the population 

variance, just use pop.var above.


R-Code:

vars = replicate(1000, var(sample(bwt.nonsmoke, 10)))

hist(vars, breaks=100)

abline(v=pop.var, col="blue")

mean(vars > pop.var * 1.5)


Question 1.2.

Now use a sample size of 50. How often (what proportion) is the sample variance 

larger than 1.5 times the population variance?


R-Code:
vars = replicate(1000, var(sample(bwt.nonsmoke, 50)))

mean(vars > pop.var * 1.5)


Plot of Sample variance and population variance

Finally, we'll make a plot to see how the sample variance estimates gets better 

(closer to the population variance) as the sample size increases. First, we'll 

make a vector of sample sizes from 2 to 400:

sample.size = 2:400

Now, for each sample size, take a sample from the nonsmokers of that size, and 

calculate the variance:

var.estimate = sapply(sample.size, function(n) var(sample(bwt.nonsmoke, n)))

Finally, plot these sample variances over their sample sizes, and draw a 

horizontal line of the population variance:

R-Code:

plot(sample.size, var.estimate)
abline(h=pop.var, col="blue")




In the video you just watched, Rafa provided some reasoning for why you might 

use a permutation test:

    "Sometimes the summary statistics we form are not as simple as just taking 

the average or the difference in averages, and ... the null distribution is not 

something we can easily compute, so permutation tests give us an alternative way 

of doing this, of computing the null distribution."

In the video, we computed the null distribution of the difference between the 

mean of the baby weights for smokers and non-smokers, using a permutation 

strategy. Let's take 50 samples from each of the two groups, and suppose that we 

want to use a permutation test to compare these:

set.seed(0)
N <- 50
smokers <- sample(babies$bwt[babies$smoke==1], N)
nonsmokers <- sample(babies$bwt[babies$smoke==0], N)

In the video, we calculated the observed difference in means:

obs <- mean(smokers) - mean(nonsmokers)

Finally, we used 1000 replicated permutations to generate a null distribution. 

We joined the two groups, shuffled the samples, and took the first 50 in one 

group, and the second 50 in a second group:

avgdiff <- replicate(1000, {
    all <- sample(c(smokers,nonsmokers))
    smokersstar <- all[1:N]
    nonsmokersstar <- all[(N+1):(2*N)]
    return(mean(smokersstar) - mean(nonsmokersstar))
})

Finally, we calculated a probability of seeing such a large difference between 

means under the null hypothesis. We use the absolute value, because we are 

interested if the difference was large in the positive or negative direction.

mean(abs(avgdiff) > abs(obs))

Question 2.1

However, the difference in means (if we assume the data is normal) has some nice 

asymptotic results which allow us to use the t-distribution to calculate the 

probability of seeing so large a difference.

Suppose we are really interested in the medians of the groups, not the means. 

And suppose we want to know if the difference in medians is more than expected 

if the two groups are actually sampled from the same population (that is, under 

the null hypothesis). Here is a case where the permutation test can easily be 

modified to give us an answer.

Use a permutation test with 1000 replications to generate a p-value for the 

observed difference in medians. What is the p-value for the two groups of 50 

defined above?


R-Code:

obs = median(smokers) - median(nonsmokers)

avgdiff = replicate(1000, {

all = sample(c(smokers,nonsmokers))

smokersstar = all[1:N]

nonsmokersstar = all[(N+1):(2*N)]

return(median(smokersstar) - median(nonsmokersstar))

})

mean(abs(avgdiff) > abs(obs))
